{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/git_repos/si650-final-project/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ingredient_preprocessor as ip\n",
    "import ingredient_indexing as ingredient_indexing\n",
    "from food_ranker import *\n",
    "import food_indexing as food_indexing\n",
    "import food_preprocessor as fp\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build ingredient tokenizer, stopwords, food tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test sentence', 'with a comma', 'chicken breast']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredient_tokenizer = ip.SplitTokenizer()\n",
    "ingredient_tokenizer.tokenize(\"This is a test sentences, with a comma...., chicken breasts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords collected 610'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set()\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for stopword in file:\n",
    "        stopwords.add(stopword.strip())\n",
    "f'Stopwords collected {len(stopwords)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'cleanedRecipes.jsonl'\n",
    "# stopwords = {'and', 'the', 'or', 'is', 'for'}\n",
    "text_key = 'NER'\n",
    "doc_augment_dict = {}\n",
    "preprocessor = fp.RegexTokenizer('\\w+', lowercase=True, multiword_expressions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY LOADING INVERTED INDEXES FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_index = ingredient_indexing.InvertedIndex()\n",
    "ingredient_index.load('ingredient_index')\n",
    "\n",
    "food_index = food_indexing.InvertedIndex()\n",
    "food_index.load('food_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR CREATE THEM IF NOT SAVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = RegexTokenizer('\\w+', lowercase=True, multiword_expressions=None)\n",
    "ingredient_index = ingredient_indexing.Indexer.create_index(ingredient_indexing.IndexType.InvertedIndex, dataset_path='cleanedRecipes.jsonl', document_preprocessor=ingredient_tokenizer, stopwords=stopwords, minimum_word_frequency=0, text_key='NER', max_docs=1000000)\n",
    "print(ingredient_index.get_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_index = food_indexing.Indexer.create_index(food_indexing.IndexType.InvertedIndex, dataset_path='cleanedRecipes.jsonl', document_preprocessor=preprocessor, stopwords=stopwords, minimum_word_frequency=10, text_key='directions', max_docs=1000000)\n",
    "print(food_index.get_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_index.save('ingredient_index')\n",
    "food_index.save('food_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term_freq': 41926, 'term_total_count': 42853}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_index.get_term_metadata('chicken')\n",
    "ingredient_index.get_term_metadata('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10800, 1],\n",
       " [14591, 1],\n",
       " [74649, 1],\n",
       " [251959, 1],\n",
       " [267836, 1],\n",
       " [330864, 1],\n",
       " [359552, 1],\n",
       " [436990, 1],\n",
       " [465951, 1],\n",
       " [530753, 1],\n",
       " [624002, 1],\n",
       " [657903, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topQ = ingredient_index.get_postings('king')\n",
    "ingredient_index.get_postings('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in topQ[:2]:\n",
    "    with open('cleanedRecipes.jsonl') as f:\n",
    "        for line in (f):\n",
    "            json_record = json.loads(line)\n",
    "            if q[0] == json_record['recipeID']:\n",
    "                print(json_record)\n",
    "                json_record['ingredients'] = json.loads(json_record['ingredients'])\n",
    "                json_record['NER'] = json.loads(json_record['NER'])\n",
    "                print(json_record)\n",
    "                print(json_record['ingredients'][0])\n",
    "                # print(json_record['directions'])\n",
    "               \n",
    "                # print(json_record['title'])\n",
    "\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([word for word in ingredient_index.index.keys() if len(word.split(\" \")) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the ranker (this can run again to get edited ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import food_ranker\n",
    "reload(food_ranker)\n",
    "from food_ranker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker(food_index, ingredient_index, preprocessor, ingredient_tokenizer, stopwords, BM25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run some test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topq = ranker.query(query_ingr='pie, flour, cream, apples, blueberries', query_freetext='sweet and spicy pie', query_NOT='eggs, pecans, nuts, almonds')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(425925, 26.335159904168286),\n",
       " (8257, 25.871304952472194),\n",
       " (408763, 24.97018147429639),\n",
       " (293926, 24.80657578140738),\n",
       " (124486, 24.50560801528211),\n",
       " (568638, 23.974394167172946),\n",
       " (227756, 23.85410134676416),\n",
       " (570966, 23.665431180690955),\n",
       " (233308, 23.53992587615707),\n",
       " (73794, 23.533914787275663)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET'S RUN SOME METRIC TESTS WITH OUR ANNOTATED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build a doc id to doc info for quick loading in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_recipe = {}  # make doc id to info dict for quick loading\n",
    "id_to_recipe_path = 'id_to_recipe.json'\n",
    "\n",
    "with open(dataset_path, 'r') as recipes_read:\n",
    "    for recipe in recipes_read:\n",
    "        recipe = json.loads(recipe)\n",
    "        recipe_id = recipe['recipeID']\n",
    "        recipe_title = recipe['title']\n",
    "        recipe_link = recipe['link']\n",
    "        id_to_recipe[recipe_id] = (recipe_title, recipe_link)\n",
    "\n",
    "with open(id_to_recipe_path, 'w') as json_out:\n",
    "    json_data = json.dumps(id_to_recipe, indent=4)\n",
    "    \n",
    "    json_out.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si650-final-project",
   "language": "python",
   "name": "si650-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
